name: Daily Business Listings Automation

on:
  schedule:
    - cron: "0 2 * * *"   # daily at 2 AM UTC
  workflow_dispatch:

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            curl-cffi==0.7.3 \
            requests \
            supabase==2.8.1 \
            colorama==0.4.6 \
            python-dotenv==1.0.1

      - name: Run scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          MAX_PAGES: "500"
          WORKERS: "10"
        run: |
          mkdir -p artifacts
          python daily_scraper.py
          # copy common outputs if they exist
          cp -f *.json artifacts/ 2>/dev/null || true
          cp -f *.csv artifacts/ 2>/dev/null || true
          cp -f scraper.log artifacts/ 2>/dev/null || true

      - name: Upload artifacts (JSON/CSV + logs)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: daily-scrape-${{ github.run_number }}
          path: artifacts/**
          retention-days: 14
